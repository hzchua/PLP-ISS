# -*- coding: utf-8 -*-
"""2_TextSummarization - LSTM_2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U51PZgjyGIVFG8kxPgHB0zJj8qg_xyPV
"""

!pip install attention
!pip install nltk
!pip install keras-self-attention
!pip install plot_keras_history

from tensorflow.keras.layers import Attention 
import numpy as np  
import pandas as pd 
import re   
import json  
import matplotlib.pyplot as plt 
import nltk
from nltk import word_tokenize, FreqDist
from nltk.corpus import stopwords   
from bs4 import BeautifulSoup 
from keras.preprocessing.text import Tokenizer 
from keras_preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords   
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from keras import backend as K 
import matplotlib.pyplot as plt
from plot_keras_history import plot_history 
from sklearn.model_selection import train_test_split
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet') 
import warnings
pd.set_option("display.max_colwidth", 200)
warnings.filterwarnings("ignore")

### Initialization Variable### 
input_path = "/content/drive/MyDrive/PLP Practice Module/df_CA_Summarization.csv"
input_sep = "^"
dataCleaned_path = '/content/drive/MyDrive/PLP Practice Module/df_CA_Summarization_cleaned.csv' 
dataCleaned_sep = ","
output_path = '/content/drive/MyDrive/PLP Practice Module/df_CA_Summarization_lstm.csv'
output_sep = ","
contraction_mapping = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have", "couldn't": "could not",
                           "didn't": "did not",  "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
                           "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",
                           "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have", "i'd": "i would",
                           "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", "i've": "i have", "isn't": "is not", "it'd": "it would",
                           "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam",
                           "mayn't": "may not", "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have",
                           "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock",
                           "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",
                           "she'd": "she would", "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", "she's": "she is",
                           "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",
                           "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", "there'd": "there would",
                           "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", "they'd've": "they would have",
                           "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", "to've": "to have",
                           "wasn't": "was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",
                           "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",
                           "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is",
                           "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",
                           "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have",
                           "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have", "y'all": "you all",
                           "y'all'd": "you all would","y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",
                           "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",
                           "you're": "you are", "you've": "you have"}
stop_words = set(stopwords.words('english')) 
max_len_text=30 
max_len_summary=15

def load_data():
  return pd.read_csv(input_path,sep=input_sep,lineterminator='\n')

def text_cleaner(text):
    newString = text.lower()
    newString = BeautifulSoup(newString, "lxml").text
    newString = re.sub(r'\([^)]*\)', '', newString)
    newString = re.sub('"','', newString) 
    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(" ")])   
    newString = re.sub(r"'s\b","",newString)
    newString = re.sub("[^a-zA-Z]", " ", newString) 
    tokens = [w for w in newString.split() if not w in stop_words]
    long_words=[]
    for i in tokens:  
        long_words.append(i)   
    return (" ".join(long_words)).strip()

def summary_cleaner(text):
    newString = re.sub('"','', text) 
    newString = re.sub(r"'s\b","",newString)
    newString = re.sub("[^a-zA-Z]", " ", newString) 
    newString = newString.lower()
    newString = newString.replace("summary text","")
    tokens=newString.split()
    newString=''
    for i in tokens:
        if len(i)>1:                                 
            newString=newString+i+' '  
    return newString

def AphaNum_cleaner(text):
    newString = re.sub('"','', text) 
    newString = re.sub(r"'s\b","",newString)
    newString = newString.replace("[","")
    newString = newString.replace("]"," ")  
    newString = newString.replace(",",";")   
    newString = newString.replace("--","; ")  
    newString = newString.replace("'"," ")  
    
    newString = newString.lower() 
    tokens=newString.split()
    newString=''
    for i in tokens:
        if len(i)>1:                                 
            newString=newString+i+' '  
    return newString

def data_cleaning(data):
  data.drop_duplicates(subset=['reviewText'],inplace=True)  #dropping duplicates
  data.dropna(axis=0,inplace=True)   #dropping na
  #data['reviewText']
  cleaned_review = []
  for t in data['reviewText']:
      cleaned_review.append(text_cleaner(t))
  data['cleaned_review'] =  cleaned_review
  #data['cleaned_summary']
  cleaned_summary = []
  for t in data['sumText']:
    cleaned_summary.append(summary_cleaner(t)) 
  data['cleaned_summary']=cleaned_summary
  data['cleaned_summary'].replace('', np.nan, inplace=True)
  data.dropna(axis=0,inplace=True) 
  #data['cleaned_education']
  cleaned_text = []
  for t in data['education']:
      cleaned_text.append(text_cleaner(t)) 
  data['cleaned_education'] =  cleaned_text 
  #data['cleaned_address']
  cleaned_text = []
  for t in data['address']:
      cleaned_text.append(AphaNum_cleaner(t)) 
  data['cleaned_address'] =  cleaned_text 
  #data['reviewText']
  cleaned_text = []
  for t in data['hours']:
      cleaned_text.append(AphaNum_cleaner(t)) 
  data['cleaned_hours'] =  cleaned_text 
  #data['cleaned_categories']
  cleaned_text = []
  for t in data['categories']:
      cleaned_text.append(AphaNum_cleaner(t)) 
  data['cleaned_categories'] =  cleaned_text 
  #data['cleaned_jobs']
  cleaned_text = []
  for t in data['jobs']:
      cleaned_text.append(AphaNum_cleaner(t)) 
  data['cleaned_jobs'] =  cleaned_text 
  #others
  data['jobs']= data['jobs'].str.replace (',',';')
  data['hours']= data['hours'].str.replace (',',';')
  data['address']= data['address'].str.replace (',',';')
  data['education']= data['education'].str.replace (',',';')
  data['categories']= data['categories'].str.replace (',',';')  
  return data

def export_data_cleaned(data_output): 
  #data_output =data_output.drop(columns =['no','jobs', 'hours', 'address', 'education', 'categories','sumText'])
  data_output =data_output.drop(columns =['sumText'])
  data_output.to_csv(dataCleaned_path, sep=dataCleaned_sep)

def load_data_cleaned(): 
  data = pd.read_csv(dataCleaned_path,sep=dataCleaned_sep,lineterminator='\n')
  data = data[data['cleaned_review'].notnull()]
  return data

def plot_byLength(data):
  text_word_count = []
  summary_word_count = []
  for i in data['cleaned_review']:
        text_word_count.append(len(i.split())) 
  for i in data['cleaned_summary']:
        summary_word_count.append(len(i.split())) 
  length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})
  length_df.hist(bins = 30)
  plt.show()

def split_train_test(data):
  data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x : 'start_ '+ x + ' _end') 
  #split our dataset into a training and validation  
  x_tr,x_val,y_tr,y_val=train_test_split(data['cleaned_review'],data['cleaned_summary'],test_size=0.1,random_state=0,shuffle=True) 
  return x_tr,x_val,y_tr,y_val

def create_model(data,x_tr,x_val,y_tr,y_val): 
  #prepare a tokenizer for reviews on training data
  x_tokenizer = Tokenizer()
  x_tokenizer.fit_on_texts(list(x_tr)) 
  #convert text sequences into integer sequences
  x_tr    =   x_tokenizer.texts_to_sequences(x_tr) 
  x_val   =   x_tokenizer.texts_to_sequences(x_val) 
  #padding zero upto maximum length
  x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') 
  x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post') 
  x_voc_size   =  len(x_tokenizer.word_index) +1
  #preparing a tokenizer for summary on training data 
  y_tokenizer = Tokenizer()
  y_tokenizer.fit_on_texts(list(y_tr)) 
  #convert summary sequences into integer sequences
  y_tr    =   y_tokenizer.texts_to_sequences(y_tr) 
  y_val   =   y_tokenizer.texts_to_sequences(y_val)  
  #padding zero upto maximum length
  y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')
  y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post') 
  y_voc_size  =   len(y_tokenizer.word_index) +1
  #creating LSTM model
  K.clear_session() 
  latent_dim = 500  
  # Encoder 
  encoder_inputs = Input(shape=(max_len_text,)) 
  enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) 
  #LSTM 1 
  encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) 
  encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)  
  #LSTM 2 
  encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) 
  encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) 
  #LSTM 3 
  encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) 
  encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)  
  # Set up the decoder. 
  decoder_inputs = Input(shape=(None,)) 
  dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) 
  dec_emb = dec_emb_layer(decoder_inputs)  
  #LSTM using encoder_states as initial state
  decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) 
  decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])  
  # Attention layer 
  attention=Attention(name="attention_layer")
  attn_out=attention([decoder_outputs, encoder_outputs]) 
  # Concat attention output and decoder LSTM output 
  decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out]) 
  #Dense layer
  decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) 
  decoder_outputs = decoder_dense(decoder_concat_input)  
  # Define the model
  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)  
  model.summary() 
  #model compile
  model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy') 
  #early stopping
  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2) 
  return model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=512, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:])),x_tokenizer,y_tokenizer,encoder_inputs,encoder_outputs, state_h, state_c,latent_dim,decoder_inputs,decoder_lstm,attention,decoder_dense,dec_emb_layer,x_tr,x_val,y_tr,y_val

def interface_preparation(x_tokenizer,y_tokenizer,encoder_inputs,encoder_outputs, state_h, state_c,latent_dim,decoder_inputs,decoder_lstm,attention,decoder_dense,dec_emb_layer):
  #Interface
  reverse_target_word_index = y_tokenizer.index_word 
  reverse_source_word_index = x_tokenizer.index_word 
  target_word_index = y_tokenizer.word_index
  # encoder inference
  encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c]) 
  # decoder inference
  # Below tensors will hold the states of the previous time step
  decoder_state_input_h = Input(shape=(latent_dim,))
  decoder_state_input_c = Input(shape=(latent_dim,))
  decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim)) 
  # Get the embeddings of the decoder sequence
  dec_emb2= dec_emb_layer(decoder_inputs) 
  # To predict the next word in the sequence, set the initial states to the states from the previous time step
  decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c]) 
  #attention inference 
  attn_out_inf = attention([decoder_outputs2,decoder_hidden_state_input])
  decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf]) 
  # A dense softmax layer to generate prob dist. over the target vocabulary
  decoder_outputs2 = decoder_dense(decoder_inf_concat) 
  # Final decoder model
  decoder_model = Model(
  [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],
  [decoder_outputs2] + [state_h2, state_c2])
  return encoder_model,decoder_model,target_word_index,reverse_target_word_index,reverse_source_word_index

def decode_sequence(input_seq,encoder_model,target_word_index,decoder_model,reverse_target_word_index):
    # Encode the input as state vectors.
    e_out, e_h, e_c = encoder_model.predict(input_seq) 
    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1,1)) 
    # Chose the 'start' word as the first word of the target sequence
    target_seq[0, 0] = target_word_index['start'] 
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c]) 
        # Sample a token 
        sampled_token_index = np.argmax(output_tokens[0, -1, 2:]) + 2
        sampled_token = reverse_target_word_index[sampled_token_index] 
        if(sampled_token!='end'):
            decoded_sentence += ' '+sampled_token 
            # Exit condition: either hit max length or find stop word.
            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_summary-1)):
                stop_condition = True 
        # Update the target sequence (of length 1).
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = sampled_token_index 
        # Update internal states
        e_h, e_c = h, c 
    return decoded_sentence

def seq2summary(input_seq,target_word_index,reverse_target_word_index):
    newString=''
    for i in input_seq:
      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):
        newString=newString+reverse_target_word_index[i]+' '
    return newString 

def seq2text(input_seq,reverse_source_word_index):
    newString=''
    for i in input_seq:
      if(i!=0):
        newString=newString+reverse_source_word_index[i]+' '
    return newString

def predict(x_val,y_val,reverse_source_word_index,target_word_index,reverse_target_word_index,encoder_model,decoder_model):
  textReview  = []
  Original_summary  = []
  Predicted_summary  = [] 
  #print(x_val[0])
  #print(reverse_source_word_index)
  seq2text(x_val[0],reverse_source_word_index)
  for i in range(len(x_val)):
    textReview.append(seq2text(x_val[i],reverse_source_word_index))
    Original_summary.append(seq2summary(y_val[i] ,target_word_index,reverse_target_word_index))
    Predicted_summary.append(decode_sequence(x_val[i].reshape(1,max_len_text),encoder_model,target_word_index,decoder_model,reverse_target_word_index)) 
  df_Result = pd.DataFrame(columns=['textReview', 'Original_summary', 'Predicted_summary'])
  df_Result['textReview'] = pd.DataFrame(textReview, columns=['textReview'])
  df_Result['Original_summary'] =  pd.DataFrame(Original_summary, columns=['Original_summary'])
  df_Result['Predicted_summary'] =  pd.DataFrame(Predicted_summary, columns=['Predicted_summary']) 
  ##OUTPUT to CSV### 
  df_Result.to_csv(output_path, sep=output_sep)

def main():
  data=load_data()
  data=data_cleaning(data)
  export_data_cleaned(data)
  data=load_data_cleaned()
  plot_byLength(data)
  #print(data)
  x_tr,x_val,y_tr,y_val = split_train_test(data)
  history, x_tokenizer,y_tokenizer,encoder_inputs,encoder_outputs, state_h, state_c,latent_dim,decoder_inputs,decoder_lstm,attention,decoder_dense,dec_emb_layer,x_tr,x_val,y_tr,y_val = create_model(data,x_tr,x_val,y_tr,y_val)
  plot_history(history)
  encoder_model,decoder_model,target_word_index,reverse_target_word_index,reverse_source_word_index= interface_preparation(x_tokenizer,y_tokenizer,encoder_inputs,encoder_outputs, state_h, state_c,latent_dim,decoder_inputs,decoder_lstm,attention,decoder_dense,dec_emb_layer)
  #print (x_val)
  predict(x_val,y_val,reverse_source_word_index,target_word_index,reverse_target_word_index,encoder_model,decoder_model)

#call main
main()